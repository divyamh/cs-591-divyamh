{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System for Amazon Electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be working with the [Amazon dataset](http://cs-people.bu.edu/kzhao/teaching/amazon_reviews_Electronics.tar.gz). You will build a recommender system to make predictions related to reviews of Electronics products on Amazon.\n",
    "\n",
    "Your grades will be determined by your performance on the predictive tasks as well as a brief written report about the approaches you took.\n",
    "\n",
    "This assignment should be completed **individually**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train.json** 1,000,000 reviews to be used for training. It is not necessary to use all reviews for training if doing so proves too computationally intensive. The fields in this file are:\n",
    "\n",
    "* **reviewerID** The ID of the reviewer. This is a hashed user identifier from Amazon.\n",
    "\n",
    "* **asin** The ID of the item. This is a hashed product identifier from Amazon.\n",
    "\n",
    "* **overall** The rating of reviewer gave the item.\n",
    "\n",
    "* **helpful** The helpfulness votes for the review. This has 2 subfields, 'nHelpful' and 'outOf'. The latter is the total number of votes this review received. The former is the number of those that considered the review to be helpful.\n",
    "\n",
    "* **reviewText** The text of the review.\n",
    "\n",
    "* **summary** The summary of the review.\n",
    "\n",
    "* **unixReviewTime** The time of the review in seconds since 1970."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**meta.json** Contains metadata of the items:\n",
    "\n",
    "* **asin** The ID of the item.\n",
    "\n",
    "* **categories** The category labels of the item being reviewed.\n",
    "\n",
    "* **price** The price of the item.\n",
    "\n",
    "* **brand** The brand of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pairs_Rating.txt** The pairs (reviewerID and asin) on which you are to predict ratings.\n",
    "\n",
    "**pairs_Purchase.txt** The pairs on which you are to predict whether a user purchased an item or not.\n",
    "\n",
    "**pairs_Helpful.txt** The pairs on which you are to predict helpfulness votes. A third column in this file is the total number of votes from which you should predict how many were helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helpful.json** The review data associated with the helpfulness prediction test set. The 'nHelpful' field has been removed from this data since that is the value you need to predict above. This data will only be of use for the helpfulness prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**baseline.py** A simple baseline for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rating prediction** Predict people's star ratings as accurately as possible for those (reviewerID, asin) pairs in 'pairs_Rating.txt'. Accuracy will be measured in terms of the [root mean-squared error (RMSE)](http://www.kaggle.com/wiki/RootMeanSquaredError).\n",
    "\n",
    "**Purchase prediction** Predict given a (reviewerID, asin) pair from 'pairs_Purchase.txt' whether the user purchased the item (really, whether it was one of the items they reviewed). Accuracy will be measured in terms of the [categorization accuracy](http://www.kaggle.com/wiki/HammingLoss) (1 minus the Hamming loss).\n",
    "\n",
    "**Helpfulness prediction** Predic whether a user's review of an item will be considered helpful. The file 'pairs_Helpful.txt' contains (reviewerID, asin) pairs with a third column containing the number of votes the user's review of the item received. You must predict how many of them were helpful. Accuracy will be measured in terms of the total [absolute error](http://www.kaggle.com/wiki/AbsoluteError), i.e. you are penalized one according to the difference |nHelpful - prediction|, where 'nHelpful' is the number of helpful votes the review actually received, and 'prediction' is your prediction of this quantity.\n",
    "\n",
    "We set up competitions on Kaggle to keep track of your results compared to those of other members of the class. The leaderboard will show your results on half of the test data, but your ultimate score will depend on your predictions across the whole dataset.\n",
    "* Kaggle competition: [rating prediction](https://inclass.kaggle.com/c/cs591-hw3-rating-prediction3) click here to [join](https://kaggle.com/join/datascience16rating)\n",
    "* Kaggle competition: [purchase prediction](https://inclass.kaggle.com/c/cs591-hw3-purchase-prediction) click here to [join](https://kaggle.com/join/datascience16purchase)\n",
    "* Kaggle competition: [helpfulness prediction](https://inclass.kaggle.com/c/cs591-hw3-helpful-prediction) click here to [join](https://kaggle.com/join/datascience16helpful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be graded on the following aspects.\n",
    "\n",
    "* Your written report. This should describe the approaches you took to each of the 3 tasks. To obtain good performance, you should not need to invent new approaches (though you are more than welcome to) but rather you will be graded based on your decision to apply reasonable approaches to each of the given tasks. (**10pts** for each task)\n",
    "\n",
    "* Your ability to obtain a solution which outperforms the baselines on the unseen portion of the test data. Obtaining full marks requires a solution which is substantially better (at least several percent) than baseline performance. (**10pts** for each task)\n",
    "\n",
    "* Your ranking for each of the three tasks compared to other students in the class. (**5pts** for each task)\n",
    "\n",
    "* Obtain a solution which outperforms the baselines on the seen portion of the test data (the leaderboard). \n",
    "(**5pts** for each task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple baselines have been provided for each of the 3 tasks. These are included in 'baselines.py' among the files above. These 3 baselines operate as follows:\n",
    "\n",
    "**Rating prediction** Returns the global average rating, or the user's average if you have seen them before in the training data.\n",
    "\n",
    "**Purchase prediction** Finds the most popular products that account for 50% of purchases in the training data. Return '1' whenever such a product is seen at test time, '0' otherwise.\n",
    "\n",
    "** Helpfulness prediction** Multiplies the number of votes by the global average helpfulness rate, or the user's rate if we saw this user in the training data.\n",
    "\n",
    "Running 'baseline.py' produces 3 files containing predicted outputs. Your submission files should have the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image-based recommendations on styles and substitutes** J. McAuley, C. Targett, J. Shi, A. van den Hengel *SIGIR*, 2015\n",
    "\n",
    "**Inferring networks of substitutable and complementary products** J. McAuley, R. Pandey, J. Leskovec *Knowledge Discovery and Data Mining*, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def readJson(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "allRatings = []\n",
    "userRatings = defaultdict(list)\n",
    "itemRatings = defaultdict(list)\n",
    "for l in readJson('train.json'):\n",
    "  user,item = l['reviewerID'],l['asin']\n",
    "  allRatings.append(l['overall'])\n",
    "  userRatings[user].append(l['overall'])\n",
    "  itemRatings[item].append(l['overall'])\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)\n",
    "'''\n",
    "# Stocastic Gradient Desecnt\n",
    "allRatings = []\n",
    "userRating = {}\n",
    "UserIDD = {}\n",
    "ItemIDD = {}\n",
    "userid = {}\n",
    "itemid = {}\n",
    "uc=0\n",
    "ic=0\n",
    "\n",
    "for line in readJson('train.json'):\n",
    "    user,item,stars= line['reviewerID'],line['asin'],line['overall']\n",
    "    train =[user,item,stars]\n",
    "    allRatings.append(train)\n",
    "\n",
    "review = pd.DataFrame(allRatings, columns = ['UserID','ItemID','Stars'])\n",
    "\n",
    "# initially taking random values for the biases and P and Q\n",
    "k=12\n",
    "userbais = np.random.rand(len(review.UserID.unique()))\n",
    "itembais = np.random.rand(len(review.ItemID.unique()))\n",
    "P = np.random.rand(len(review.UserID.unique()),k)\n",
    "Q = np.random.rand(len(review.ItemID.unique()),k)\n",
    "Q = Q.T\n",
    "\n",
    "Lamda = 1 \n",
    "\n",
    "LRate = 0.05\n",
    "\n",
    "for userID in review.UserID.unique():\n",
    "    UserIDD[userID]=uc\n",
    "    uc+=1\n",
    "    \n",
    "for itemID in review.ItemID.unique():\n",
    "    ItemIDD[itemID]=ic\n",
    "    ic+=1\n",
    "\n",
    "review['UserID'] = review['UserID'].apply(lambda x:int(UserIDD[x]))\n",
    "review['ItemID'] = review['ItemID'].apply(lambda x:int(ItemIDD[x]))\n",
    "\n",
    "for k in range(6):\n",
    "    Global_mu = globalAverage\n",
    "    for k in range(len(review)):\n",
    "        UserID = review['UserID'][k]\n",
    "        ItemID = review['ItemID'][k]\n",
    "        Error = review['Stars'][k]-(Global_mu+userbais[UserID]+itembais[ItemID]+np.dot(P[UserID,:] , Q[:,ItemID]))\n",
    "        userbais[UserID] += (LRate*(Error-(Lamda*userbais[UserID])))\n",
    "        itembais[ItemID] += (LRate*(Error-(Lamda*itembais[ItemID])))\n",
    "        q = Q[:,ItemID]+(LRate*(Error*P[UserID,:]-Lamda*Q[:,ItemID]))\n",
    "        p = P[UserID,:]+(LRate*(Error*Q[:,ItemID]-Lamda*P[UserID,:]))\n",
    "        # to store the old value\n",
    "        Q[:,ItemID]=q\n",
    "        P[UserID,:]=p\n",
    "\n",
    "        \n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "userid = {}\n",
    "itemid = {}\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "  if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  for k in range(len(review)):\n",
    "        userid = review['UserID'][k]\n",
    "        itemid = review['ItemID'][k]\n",
    "        try:\n",
    "            if itemid in review['ItemID'] and userid in review['UserID']:\n",
    "                pred = Global_mu+userbais[UserIDD[userid]]+itembais[ItemIDD[itemid]]+np.dot(P[UserIDD[userid],:],Q[:,ItemIDD[itemid]])\n",
    "                predictions.write(u + '-' + i + ',' + str(pred) + '\\n')\n",
    "            else:\n",
    "                predictions.write(u + '-' + i + ',' + str(Global_mu) + '\\n')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "userAverage = {}\n",
    "itemAverage = {}\n",
    "for u in userRatings:\n",
    "    userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "for v in itemRatings:\n",
    "    itemAverage[v] = sum(itemRatings[v]) / len(itemRatings[v])\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "  if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  if i in itemAverage and u in userAverage:\n",
    "    avg = (2*itemAverage[i] + userAverage[u] + globalAverage)/4    \n",
    "    predictions.write(u + '-' + i + ',' + str(avg) + '\\n')\n",
    "  elif i in itemAverage and u not in userAverage:\n",
    "    avg = (itemAverage[i] + globalAverage)/2 \n",
    "    predictions.write(u + '-' + i + ',' + str(avg) + '\\n')\n",
    "  elif u in userAverage and i not in itemAverage:\n",
    "    avg = (userAverage[u] + globalAverage)/2\n",
    "    predictions.write(u + '-' + i + ',' + str(avg) + '\\n')\n",
    "  else:\n",
    "    predictions.write(u + '-' + i + ',' + str(globalAverage) + '\\n')\n",
    "predictions.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 was to predict user’s ratings for the file provided. Initially I tried to build a recommendation system using stochastic gradient descent and predicting from it. First I read all the data from the train.json file and created a data frame ‘review’ which contents all the userid, itemid and the ratings they gave. I took my k=12 and initially calculated random user bias and item bias both of which where the size of unique users and unique items, I took lambda to be 1 and the learning rate to be 0.05. I calculate the global average and put it in place of Mu. Finally I trained my system using 6 iterations and updated the error and the bias accordingly ever time using the equation taught. I also included the dot product. Then I opened the prediction file, read the pair from their and and tried to predict the rating from the Mu, user bias, item bias and the P and Q values as seen in the formula thought in the class. Although the program ran but it took hours to compute and the score on the kaggle completion for this solution was very bad, it hardly just beat the baseline. I have included the code for this above in the commented section.\n",
    "\n",
    "Finally, not getting the desired output from the above method, I calculated the global average of all the ratings and gave the values somewhat like this: \n",
    "1)\tif both the item and the user is present in our training dataset then I have taken the average of all of that particular user’s ratings that we have, the average of all of that particular item’s ratings and the global average. I have given a little more weightage to the item’s average as it makes more sense to get similar ratings for the same item.\n",
    "2)\tIf only the item is present in our training dataset and the user is not present, then I’ve computed the average of the item’s average and the global average and written that.\n",
    "3)\tIf only the user is present in our training dataset and the item is not present, then I’ve computed the average of the user’s ratings average and the global average and written that.\n",
    "4)\tIf neither the user nor the item is present in our training dataset, then I’ve just taken the global average and written that as we don’t have any other info on it.\n",
    "Surprisingly the method above gave me a very good score and thus I’ve submitted these scores as my final score on kaggle. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def readJson(f):\n",
    "  for l in open(f):\n",
    "    yield eval(l)\n",
    "'''\n",
    "\n",
    "countVector = CountVectorizer(min_df=1)\n",
    "list = []\n",
    "for line in readJson('meta.json'):\n",
    "    item,categories = line['asin'],line['categories']\n",
    "    list.append([item,categories])\n",
    "    \n",
    "df = pd.DataFrame(list,columns=['item','category'])\n",
    "itemsdf = pd.DataFrame(countVector.fit_transform(df.category).toarray(), columns=countVector.get_feature_names())\n",
    "\n",
    "mis = np.zeros(30)\n",
    "mis[0] = 0;\n",
    "for key in range(1,30):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=key, n_init=100)\n",
    "    kmeans.fit_predict(itemsdf)\n",
    "    mis[key] = kmeans.inertia_\n",
    "\n",
    "plt.plot(range(1,len(mis)),mis[1:])\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=20, n_init=100)\n",
    "kmeans.fit_predict(itemsdf)\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "mis = kmeans.inertia_\n",
    "\n",
    "'''\n",
    "itemCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "\n",
    "for l in readJson('train.json'):\n",
    "  user,item = l['reviewerID'],l['asin']\n",
    "  itemCount[item] += 1\n",
    "  totalPurchases += 1\n",
    "\n",
    "mostPopular = [(itemCount[x], x) for x in itemCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular[:61110]:\n",
    "  count += ic\n",
    "  return1.add(i)\n",
    " \n",
    "\n",
    "predictions = open(\"predictions_Purchase.txt\", 'w')\n",
    "for l in open(\"pairs_Purchase.txt\"):\n",
    "  if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  if i in return1:\n",
    "    predictions.write(u + '-' + i + \",1\\n\")\n",
    "  else:\n",
    "    predictions.write(u + '-' + i + \",0\\n\")\n",
    "predictions.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 was to predict weather user purchased the item or not. Initially I tried to solve this using clusters. I used k-means ++ and formed clusters of similar items. For doing so I created a list and read all the items and categories from the meta file, then I converted it to a data frame where I had all the items with all of these respective categories. Based on which I formed 20 clusters. My idea was that if a user bought an item from a cluster then his chances of buying another similar item which would belong to the same cluster was more likely. So for evry pair that I read from the precidtion file, I would check which cluster the item belonged to and if the user has bought any item from that particular cluster, if the user has bought an item before then I would give the value 1 else if not then 08. But this didn’t work well, the clusetering is taking hours and the kernel crashes for many functions that I try to perform. I have included the code for forming the clusters above.\n",
    "\n",
    "Thus following which to improve on the baseline I checked that the top 61,110 items sold are the once that have the maximum frequencies of purchase. So, I created an list of the MostPopular items sold and sorted this list by descending order of the number of this item sold and picked the top 61,110 items sold out of the 170,000 items. Following which I gave 1 if the item belonged to this list else 0. This approach gave me a very good score on the kaggle competition and so I left it at that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readJson(f):\n",
    "  for l in open(f):\n",
    "    yield eval(l)\n",
    "\n",
    "allHelpful = []\n",
    "userHelpful = defaultdict(list)\n",
    "itemHelpful = defaultdict(list)\n",
    "\n",
    "for l in readJson('train.json'):\n",
    "  user,item = l['reviewerID'],l['asin']\n",
    "  allHelpful.append(l['helpful'])\n",
    "  userHelpful[user].append(l['helpful'])\n",
    "  itemHelpful[item].append(l['helpful'])  \n",
    "\n",
    "averageRate = sum([x['nHelpful'] for x in allHelpful]) * 1.0 / sum([x['outOf'] for x in allHelpful])\n",
    "userRate = {}\n",
    "itemRate = {}\n",
    "for u in userHelpful:\n",
    "  userRate[u] = sum([x['nHelpful'] for x in userHelpful[u]]) * 1.0 / sum([x['outOf'] for x in userHelpful[u]])\n",
    "for i in itemHelpful:\n",
    "  itemRate[i] = sum([x['nHelpful'] for x in itemHelpful[i]]) * 1.0 / sum([x['outOf'] for x in itemHelpful[i]])\n",
    "\n",
    "values = userRate.values()\n",
    "useravg = sum(values) / len(values)\n",
    "\n",
    "values = itemRate.values()\n",
    "itemavg = sum(values) / len(values)\n",
    "\n",
    "predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "for l in open(\"pairs_Helpful.txt\"):\n",
    "  if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i,outOf = l.strip().split('-')\n",
    "  outOf = int(outOf)\n",
    "  if i in itemRate and u in userRate:\n",
    "    avg = (2*userRate[u] + itemRate[i] + averageRate)/4\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*avg) + '\\n')  \n",
    "  elif u not in userRate and i in itemRate:\n",
    "    avg = (itemRate[i] + averageRate)/2\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*avg) + '\\n')\n",
    "  elif u in userRate and i not in itemRate:\n",
    "    avg = (userRate[u] + averageRate)/2\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*avg) + '\\n')\n",
    "  else:\n",
    "    avg = (averageRate)\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(outOf*avg) + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 was to predict how many out of the total votes found a particular review helpful. I initially started with the same approach as the first task. Just instead of teaching the review stars I was passing the value I got after dividing the number of helpful votes by the total number of votes. I tried to form a recommendation system using stochastic gradient decent and predicted from it using the formula. But as the approach took a very long time and the output wasn’t even very good in its score at the end solved this question using the similar average method I used for task 1. \n",
    "I calculated the global average of all the helpful votes divided by all the total number of votes for each review, also calculated average of all the user’s helpfulness and all the item’s helpfulness and gave the values somewhat like this: \n",
    "1)\tif both the item and the user is present in our training dataset then I have taken the average of all of that particular user’s ratings helpfulness that we have, the average of all of that particular item’s review’s helpfulness and the global average of helpfulness. I have given a little more weightage to the item’s helpfulness as it makes more sense to get similar votes for the same item.\n",
    "2)\tIf only the item is present in our training dataset and the user is not present, then I’ve computed the average of the item’s rating’s helpfulness average, the global average helpfulness and written that.\n",
    "3)\tIf only the user is present in our training dataset and the item is not present, then I’ve computed the average of the user’s ratings helpfulness average and the global average helpfulness and written that.\n",
    "4)\tIf neither the user nor the item is present in our training dataset, then I’ve just taken the global average helpfulness and written that as we don’t have any other info on it.\n",
    "Finally, I multiplied the average scores by the number of total votes (Outof) that we have for each review as it showed up on the prediction text file.\n",
    "Surprisingly the method above gave me a very good score and thus I’ve submitted these scores as my final score on kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\r\n",
       "<style>\r\n",
       "    @font-face {\r\n",
       "        font-family: \"Computer Modern\";\r\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "    }\r\n",
       "    .code_cell {\r\n",
       "        width: 105ex !important ;\r\n",
       "        margin-bottom: 15px !important;\r\n",
       "    }\r\n",
       "    div.cell {\r\n",
       "        margin-left: auto;\r\n",
       "        margin-right: auto;\r\n",
       "        width: 70%;\r\n",
       "    }    \r\n",
       "    div.cell.selected {\r\n",
       "        border: thin rgba(171, 171, 171, 0.5) dashed;\r\n",
       "    }\r\n",
       "    h1 {\r\n",
       "        font-family: 'Alegreya Sans', sans-serif;\r\n",
       "    }\r\n",
       "    h2 {\r\n",
       "        font-family: 'EB Garamond', serif;\r\n",
       "    }\r\n",
       "    h3 {\r\n",
       "        font-family: 'EB Garamond', serif;\r\n",
       "        margin-top:12px;\r\n",
       "        margin-bottom: 3px;\r\n",
       "    }\r\n",
       "    h4 {\r\n",
       "        font-family: 'EB Garamond', serif;\r\n",
       "    }\r\n",
       "    h5 {\r\n",
       "        font-family: 'Alegreya Sans', sans-serif;\r\n",
       "    }\r\n",
       "    div.text_cell_render {\r\n",
       "        font-family: 'EB Garamond',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\r\n",
       "        line-height: 145%;\r\n",
       "        font-size: 140%;\r\n",
       "    }\r\n",
       "    div.input_area {\r\n",
       "        border-color: rgba(0,0,0,0.10) !important;\r\n",
       "        background: #fafafa;\r\n",
       "    }\r\n",
       "    .CodeMirror {\r\n",
       "            font-family: \"Source Code Pro\";\r\n",
       "            font-size: 90%;\r\n",
       "    }\r\n",
       "    .prompt {\r\n",
       "        display: None;\r\n",
       "    }\r\n",
       "    .output {\r\n",
       "        padding-left: 50px;\r\n",
       "        padding-top: 5px;\r\n",
       "    }\r\n",
       "    .output_wrapper {\r\n",
       "        padding-left: 5px;\r\n",
       "        padding-top: inherit;\r\n",
       "    }\r\n",
       "    div.output_scroll {\r\n",
       "        width: inherit;\r\n",
       "    }\r\n",
       "    .inner_cell {\r\n",
       "        padding-left: 5px;\r\n",
       "    }\r\n",
       "    .text_cell_render h1 {\r\n",
       "        font-weight: 200;\r\n",
       "        font-size: 50pt;\r\n",
       "        line-height: 100%;\r\n",
       "        color:#CD2305;\r\n",
       "        margin-bottom: 0.5em;\r\n",
       "        margin-top: 0.5em;\r\n",
       "        display: block;\r\n",
       "    }\r\n",
       "    .text_cell_render h5 {\r\n",
       "        font-weight: 300;\r\n",
       "        font-size: 16pt;\r\n",
       "        color: #CD2305;\r\n",
       "        font-style: italic;\r\n",
       "        margin-bottom: .5em;\r\n",
       "        margin-top: 0.5em;\r\n",
       "        display: block;\r\n",
       "    }\r\n",
       "    .warning {\r\n",
       "        color: rgb( 240, 20, 20 )\r\n",
       "        }  \r\n",
       "</style>\r\n",
       "<script>\r\n",
       "    MathJax.Hub.Config({\r\n",
       "                        TeX: {\r\n",
       "                           extensions: [\"AMSmath.js\"]\r\n",
       "                           },\r\n",
       "                tex2jax: {\r\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\r\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\r\n",
       "                },\r\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\r\n",
       "                \"HTML-CSS\": {\r\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\r\n",
       "                }\r\n",
       "        });\r\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
